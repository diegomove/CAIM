{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2L1HwcMeBtiW"
   },
   "source": [
    "In this session you will:\n",
    "\n",
    "- learn the basics of Spark's RDD API\n",
    "- implement Mapreduce algorithms for inverted indices and Pagerank, and reason about their relative efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLX0kIryBtiX"
   },
   "source": [
    "# 1. Getting started with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Spark locally\n",
    "It comes with a default local implementation.\n",
    "You may want to have a look at http://localhost:4040/jobs/ after staring a Spark job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.0' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/usuario/AppData/Local/Programs/Python/Python313/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "#!pip3 install pyspark --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 12 cores\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# local[*] runs locally using all available cores; replace with local[1] to use only one core\n",
    "spark = SparkSession.builder.master(\"local[*]\").config(\"spark.driver.memory\", \"8g\").getOrCreate()           \n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(\"Using %i cores\" % sc.defaultParallelism)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load text into an RDD\n",
    "A spark RDD (resilient distributed dataset) is a fault-tolerant collection that can be distributed across multiple cluster nodes.\n",
    "\n",
    "RDDs are immutable (cannot be modified), but we can use an RDD to derive another RDD from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A mapper mapped as much data as any mapper could map,',\n",
      " 'but the reducer reduced the mapped data faster than any mapper could map.',\n",
      " 'The mapper mapped and the reducer reduced until results were produced.']\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Simple example: we pretend this is a large file split into many chunks\n",
    "text = \"\"\"A mapper mapped as much data as any mapper could map,\n",
    "but the reducer reduced the mapped data faster than any mapper could map.\n",
    "The mapper mapped and the reducer reduced until results were produced.\n",
    "\"\"\"\n",
    "lines = sc.parallelize(text.splitlines())\n",
    "\n",
    "# Each worker will receive one or more lines.\n",
    "# Now we start a Spark job to print a few of them for verification purposes\n",
    "pprint(lines.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Counting word frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "regex = re.compile(r\"[0-9]+|[a-zà-ÿ]+'?[a-zà-ÿ']*\", re.IGNORECASE)\n",
    "\n",
    "def tokens(s):\n",
    "    return [m.group(0).lower() for m in regex.finditer(s)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hola', 'carabola', 'qué', 'tal', 'ñam', 'ío', '48']\n"
     ]
    }
   ],
   "source": [
    "print(tokens(\"Hola, carabola, ¿qué tal? Ñam! Ío48\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 1 (classic Mapreduce without combiner)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapper\n",
    "- For each line, we want to output a list of (word, 1) pairs for each word in line. \n",
    "- This is an iteration over elements, producing several key-value pairs for each element.\n",
    "- We cannot directly write code for the workers; this iteration is accomplished via Spark's `flatMap` function.\n",
    "- By contrast, Spark's `map` function would only serve to produce a single output from each element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapper(line):\n",
    "    return ( (word, 1) for word in tokens(line) )\n",
    "\n",
    "words1 = lines.flatMap(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Spark uses lazy evaluation: no data has been processed yet. \n",
    "- It has only built a plan on how to transform `lines` into `words1`.\n",
    "- Actions such as `count` or `take` trigger actual computation jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 words\n",
      "[('a', 1),\n",
      " ('mapper', 1),\n",
      " ('mapped', 1),\n",
      " ('as', 1),\n",
      " ('much', 1),\n",
      " ('data', 1),\n",
      " ('as', 1),\n",
      " ('any', 1),\n",
      " ('mapper', 1),\n",
      " ('could', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"%i words\" % words1.count())\n",
    "pprint(words1.take(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle\n",
    "- This operation is handled automatically in the classical MapReduce framework, but needs to be made explicit in Spark.\n",
    "- It is accomplished via the `groupByKey` function. \n",
    "- This can be very costly if there are some very common words, as we haven't specified a combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words1_shuffled = words1.groupByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists in the shuffled RDD are returned as 'iterables'. \n",
    "For viewing purposes only, we may materialize them into Python lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('and', [1]),\n",
      " ('reduced', [1, 1]),\n",
      " ('could', [1, 1]),\n",
      " ('mapped', [1, 1, 1]),\n",
      " ('map', [1, 1]),\n",
      " ('faster', [1]),\n",
      " ('were', [1]),\n",
      " ('as', [1, 1]),\n",
      " ('a', [1]),\n",
      " ('reducer', [1, 1]),\n",
      " ('produced', [1]),\n",
      " ('than', [1]),\n",
      " ('much', [1]),\n",
      " ('any', [1, 1]),\n",
      " ('the', [1, 1, 1, 1]),\n",
      " ('until', [1]),\n",
      " ('mapper', [1, 1, 1, 1]),\n",
      " ('but', [1]),\n",
      " ('results', [1]),\n",
      " ('data', [1, 1])]\n"
     ]
    }
   ],
   "source": [
    "pprint([ (w, list(l)) for (w, l) in words1_shuffled.take(20) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reducer\n",
    "- For each (word, L), return (word, sum(L))\n",
    "- This is simply just another 'map' in the functional programming sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reducer(tup):\n",
    "    word, L = tup[0], tup[1]\n",
    "    return (word, sum(L))\n",
    "\n",
    "counts = words1_shuffled.map(reducer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4),\n",
      " ('mapper', 4),\n",
      " ('mapped', 3),\n",
      " ('reduced', 2),\n",
      " ('could', 2),\n",
      " ('map', 2),\n",
      " ('as', 2),\n",
      " ('reducer', 2),\n",
      " ('any', 2),\n",
      " ('data', 2),\n",
      " ('and', 1),\n",
      " ('faster', 1),\n",
      " ('were', 1),\n",
      " ('a', 1),\n",
      " ('produced', 1),\n",
      " ('than', 1),\n",
      " ('much', 1),\n",
      " ('until', 1),\n",
      " ('but', 1),\n",
      " ('results', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(counts.takeOrdered(20, key=lambda x:-x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "The entire pipeline could have been written much more succinctly by using anonymous functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (lines\n",
    "    .flatMap(lambda line:( (word, 1) for word in tokens(line) ))        # mapper\n",
    "    .groupByKey()                                                       # shuffle\n",
    "    .map(lambda tup:(tup[0], sum(tup[1])))                              # reducer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4),\n",
      " ('mapper', 4),\n",
      " ('mapped', 3),\n",
      " ('reduced', 2),\n",
      " ('could', 2),\n",
      " ('map', 2),\n",
      " ('as', 2),\n",
      " ('reducer', 2),\n",
      " ('any', 2),\n",
      " ('data', 2),\n",
      " ('and', 1),\n",
      " ('faster', 1),\n",
      " ('were', 1),\n",
      " ('a', 1),\n",
      " ('produced', 1),\n",
      " ('than', 1),\n",
      " ('much', 1),\n",
      " ('until', 1),\n",
      " ('but', 1),\n",
      " ('results', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(counts.takeOrdered(20, key=lambda x:-x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METHOD 2 (combiner for associative, commutative reducers)\n",
    "- In Method 1 we didn't specify a combiner. Spark is built on the philosophy that shuffle operations should be avoided as much as possible.\n",
    "- Spark assumes that most reduce operations arising in practice are associative and commutative.\n",
    "- If so, reducers do not need to take an entire list; it suffices to know how to combine two $(key, value_1)$ and $(key, value_2)$ tuples into another tuple $(key, value_3)$. Combiners can do the same.\n",
    "- Thus, we use Spark's `reduceByKey`, passing as a parameter the function that sums two values.\n",
    "- **This is the preferred way** whenever possible, as it corresponds to using a combiner and significantly reduces the amount of disk storage and network communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = (lines\n",
    "    .flatMap(lambda line:( (word, 1) for word in tokens(line) ))    # mapper\n",
    "    .reduceByKey(lambda a, b: a + b)                                # reducer and combiner; shuffles much less data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 4),\n",
      " ('mapper', 4),\n",
      " ('mapped', 3),\n",
      " ('reduced', 2),\n",
      " ('could', 2),\n",
      " ('map', 2),\n",
      " ('as', 2),\n",
      " ('reducer', 2),\n",
      " ('any', 2),\n",
      " ('data', 2),\n",
      " ('and', 1),\n",
      " ('faster', 1),\n",
      " ('were', 1),\n",
      " ('a', 1),\n",
      " ('produced', 1),\n",
      " ('than', 1),\n",
      " ('much', 1),\n",
      " ('until', 1),\n",
      " ('but', 1),\n",
      " ('results', 1)]\n"
     ]
    }
   ],
   "source": [
    "pprint(counts.takeOrdered(20, key=lambda x:-x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last step: from absolute counts to frequencies\n",
    "We can first compute the total number of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35 total words\n",
      "20 different words\n"
     ]
    }
   ],
   "source": [
    "num_words = counts.map(lambda tup:tup[1]).sum()\n",
    "print(\"%i total words\" % num_words)\n",
    "print(\"%i different words\" % counts.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Instead of Spark's sum() method, we could have used `reduce()` to compute the sum\n",
    "- `reduceByKey` would not suit here, as there are no keys anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n"
     ]
    }
   ],
   "source": [
    "print(counts.map(lambda tup:tup[1]).reduce(lambda a,b:a+b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now each worker can compute frequencies by dividing by `num_words`\n",
    "- Note that num_words is sent to every time for every task. This is fine for a small integer, but if we wanted the machines to cache this value for all tasks we would use `sc.broadcast()` instead.\n",
    "- Finally, we print the five most frequent word, along with each frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.11428571428571428),\n",
      " ('mapper', 0.11428571428571428),\n",
      " ('mapped', 0.08571428571428572),\n",
      " ('reduced', 0.05714285714285714),\n",
      " ('could', 0.05714285714285714)]\n"
     ]
    }
   ],
   "source": [
    "freqs = counts.map(lambda tup:(tup[0], tup[1] / num_words))\n",
    "pprint(freqs.takeOrdered(5, key=lambda x:-x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 0.11428571428571428),\n",
      " ('mapper', 0.11428571428571428),\n",
      " ('mapped', 0.08571428571428572),\n",
      " ('reduced', 0.05714285714285714),\n",
      " ('could', 0.05714285714285714)]\n"
     ]
    }
   ],
   "source": [
    "# Using broadcast:\n",
    "num_words_b = sc.broadcast(num_words)\n",
    "freqs = counts.map(lambda tup:(tup[0], tup[1] / num_words_b.value))\n",
    "pprint(freqs.takeOrdered(5, key=lambda x: -x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Inverted index with Mapreduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "- We read the CSV with Wikipedia titles from last lab and turn it into an RDD of (doc_id, title string) pairs.\n",
    "- We use `repartition` to specify a desired number of parts.\n",
    "- `cache` is used so that the workers cache their data locally. This avoids reading data from disk each time we want to apply a different computation on the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_rdd = (spark.read.csv(\"enwiki-2013-names.csv\", header=True, escape=\"\\\\\")\n",
    "              .na.fill({\"name\": \"\"})\n",
    "              .rdd\n",
    "              .map(lambda row: (int(row.node_id), row.name))\n",
    "              .repartition(6)\n",
    "              .cache()\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 29:============================>                            (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4206785 titles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(30, 'DAT (newspaper)'),\n",
       " (31, 'Ularbek Baitailaq'),\n",
       " (32, 'Altyn Tamyr'),\n",
       " (33, 'Tortinshi Bilik'),\n",
       " (34, 'Middle Yangchenghu Road Station'),\n",
       " (35, 'North Qimen Main Street Station'),\n",
       " (36, 'Xujiang Road Station'),\n",
       " (37, 'Laodong Road Station'),\n",
       " (38, 'Line 2, Suzhou Rail Transit'),\n",
       " (39, 'Middle Chunshenhu Road Station')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"%i titles\" % titles_rdd.count())\n",
    "titles_rdd.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# While developing your code, you may want to test it against a smaller dataset\n",
    "if False:\n",
    "    titles_rdd = sc.parallelize(zip(range(len(text.splitlines())), text.splitlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 1:**  \n",
    "Using RDDs, write a function to build an inverted index for `titles_rdd`. It should output an RDD containing all `(term, posting list)` pairs. Each non-stopword must appear exactly once in the output and the posting list has to be sorted (hint: `sortByKey()`).\n",
    "\n",
    "You may use the code below to determine a set of stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:======================================>                   (4 + 2) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('of', 312846),\n",
      " ('u', 281808),\n",
      " ('the', 195580),\n",
      " ('in', 94147),\n",
      " ('list', 80441),\n",
      " ('80', 75900),\n",
      " ('and', 63075),\n",
      " ('â', 62175),\n",
      " ('de', 43194),\n",
      " ('county', 43105),\n",
      " ('a', 40734),\n",
      " ('school', 39807),\n",
      " ('john', 38042),\n",
      " ('93', 37596),\n",
      " ('disambiguation', 37508),\n",
      " ('station', 35116),\n",
      " ('album', 35029),\n",
      " ('district', 34713),\n",
      " ('new', 31946),\n",
      " ('film', 31256),\n",
      " ('river', 29937),\n",
      " ('at', 27697),\n",
      " ('season', 27545),\n",
      " ('national', 26785),\n",
      " ('for', 24153),\n",
      " ('s', 22295),\n",
      " ('st', 22012),\n",
      " ('song', 22005),\n",
      " ('united', 21919),\n",
      " ('william', 21387),\n",
      " ('football', 21340),\n",
      " ('ã', 21338),\n",
      " ('c', 21292),\n",
      " ('state', 21233),\n",
      " ('9', 20205),\n",
      " ('high', 19954),\n",
      " ('world', 19588),\n",
      " ('8', 18919),\n",
      " ('n', 18789),\n",
      " ('south', 18716)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Find stopwords\n",
    "counts = (titles_rdd\n",
    "    .flatMap(lambda tup:( (word, 1) for word in tokens(tup[1]) ))        # mapper\n",
    "    .reduceByKey(lambda a, b: a + b)                                     # reducer and combiner\n",
    ")        \n",
    "stopwords_freq = counts.takeOrdered(40, key=lambda x:-x[1])\n",
    "pprint(stopwords_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Pagerank with Mapreduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load graph as an RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_rdd = (sc.textFile(\"enwiki-2013.txt\")\n",
    "    .filter(lambda s: s[0] != '#')\n",
    "    .map(lambda s: tuple(map(int, s.split())))\n",
    "    .cache()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 49:=====================================================>  (45 + 2) / 47]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded graph: 4206785 vertices, 101311613 directed edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "[Stage 49:======================================================> (46 + 1) / 47]\r\n",
      "\r\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "m = edges_rdd.count()\n",
    "n = max(titles_rdd.count(), edges_rdd.map(max).max() + 1)\n",
    "print(\"Loaded graph: %i vertices, %i directed edges\" % (n, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Exercise 2:**  \n",
    "Using RDDs, write a function to compute Pagerank with damping. We assume that each worker has $\\Theta(n)$ memory (enough to store a full Pagerank or degree vector, but not enough to store the complete adjacency matrix).\n",
    "\n",
    "Some subtasks you will need to solve with Spark are:\n",
    "- Compute an RDD with the out degree of all nodes.\n",
    "- Compute the probabilities resulting from a single random step in the graph, given the current `pr` vector.\n",
    "\n",
    "The rest (stopping, damping and teleportation, etc.) is handled by the driver (main program).\n",
    "    \n",
    "Additional **questions to be discussed in the report**:\n",
    "\n",
    "1. The teleportation logic could be simplified if we computed the Google matrix in advance. What would happen then?\n",
    "2. Compare with your sequential solution from lab 6. Are they the same? Which one is faster? Why do you think that's the case?\n",
    "3. Suppose we precomputed an RDD with the list of incoming neighbors for each vertex using `groupByKey` and used it in the main loop, instead of summing contributions of individual edges. Is this approach always faster or can it become problematic?\n",
    "4. What if $n$ were too big to store the entire Pagerank/degree vectors in memory? What would you do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code skeleton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given the outdegrees rdd, returns the list of dead-end nodes, \n",
    "# and a modified out-degree vector where their degree is 1.\n",
    "def get_sinks(outdeg_rdd):\n",
    "    # Collect the out degrees into the driver program's memory\n",
    "    outdeg = np.zeros(n)\n",
    "    for (key, val) in outdeg_rdd.collect():\n",
    "        outdeg[key] = val\n",
    "\n",
    "    sinks = np.where(outdeg == 0)[0]\n",
    "    print(\"%i dead-end nodes\" % len(sinks))\n",
    "    outdeg[sinks] = 1                            # avoid division by zero\n",
    "    return sinks, outdeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_outdegrees_rdd(edges_rdd):\n",
    "    # ... your code, using RDDs\n",
    "\n",
    "def pagerank(edges_rdd, n, damping, teleport=None, tol=1e-5, max_iters=100):\n",
    "    # Pagerank vector, initially uniform\n",
    "    pr = np.full(n, 1.0 / n)\n",
    "\n",
    "    # Teleport vector, uniform if not provided\n",
    "    teleport =  # ... your code\n",
    "    # while not (termination condition):      # ... your code\n",
    "        pr_nowhere = # ... your code\n",
    "        pr_teleport = # ... your code\n",
    "\n",
    "        # Compute probabilities without teleportation at the next step\n",
    "        pr_divided = sc.broadcast(pr / outdeg)\n",
    "        step =  # ... you code, using RDDs\n",
    "\n",
    "        # Now account for teleportation\n",
    "        pr = np.full(n, pr_teleport / n)\n",
    "        for key, val in step.collect():\n",
    "            pr[key] += # ... your code\n",
    "            \n",
    "    return pr\n",
    "\n",
    "sinks, outdeg = get_sinks(compute_outdegrees(edges_rdd))\n",
    "pr = pagerank(edges_rdd, n, damping=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_topk(pr, titles_rdd, topk=20):\n",
    "    print(\"Top %i nodes in order of decreasing pagerank:\" % topk)\n",
    "    top = np.argsort(-pr)[:topk]\n",
    "    f = titles_rdd.filter(lambda tup:tup[0] in top).cache()\n",
    "    pprint([ (pr[x], f.lookup(x)[0]) for x in top ])\n",
    "    \n",
    "show_topk(pr, titles_rdd)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Rules of delivery\n",
    "\n",
    "- To be solved in _pairs_.\n",
    "\n",
    "- Submit the **report** as a PDF file. Make sure it has **both your names, date, and title**. Include your **code** in your submission (.py or .ipynb).\n",
    "\n",
    "- No plagiarism; don't discuss your work with other teams. You can ask for help to others for simple things, such as recalling a python instruction or module, but nothing too specific to the session.\n",
    "\n",
    "- If you feel you are spending much more time than the rest of the classmates, ask us for help. Questions can be asked either in person or by email, and you'll never be penalized by asking questions, no matter how stupid they look in retrospect.\n",
    "\n",
    "- Write a short report listing the solutions to the exercises proposed. Include things like the important parts of your implementation (data structures used for representing objects, algorithms used, etc). You are welcome to add conclusions and findings that depart from what we asked you to do. We encourage you to discuss the difficulties you find; this lets us give you help and also improve the lab session for future editions.\n",
    "\n",
    "\n",
    "\n",
    "- Submit your work through the [raco](http://www.fib.upc.edu/en/serveis/raco.html); see date at the raco's submissions page."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "66px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
